{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YANN (Yet Another Neural Network) Backpropogation Example\n",
    "## 1. Introduction\n",
    "This document demonstrates the mechanics, underlying a simple but not completely simplistic fully connected neural network (NN).\n",
    "In particular the multivariable mathematics involved in backpropogation is illustrated with a running example, which eschews the irritating handwavy \n",
    "approach found in many books, all of which seem to assume you can't differentiate (some of these texts do cover differentiation but they assume the\n",
    "single variable case, while even simple NNs are multivariate in real life). The example will utilise matrix multiplication of Jacobians for implementing\n",
    "the chain rule. \n",
    "\n",
    "In order to validate our results along the way a (low level tensor based) Pytorch implementation is used to output gradients and final weights\n",
    "and these values are utilised to check the mathematics. After completing the mathematical description, the example network is also implemented in the higher level API \n",
    "available in PyTorch, along with a low and high level implementation in Tensorflow 2, with the low level implementation utilising the GradientTape API while the \n",
    "higher level version is implemented using the new tf.keras API.\n",
    "\n",
    "We start with an image illustrating the example network:\n",
    "![Example Neural Network](NN.png \"Example Neural Network\")\n",
    "\n",
    "It comprises 3 inputs, two hidden layers of size 3 and 2 respectively with two outputs evaluated by a [Binary CrossEntropy loss function](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a). The first hidden layer has a RELU activation function while the second uses a Sigmoid to provide probabilities. \n",
    "\n",
    "The document is implemented the open source [SageMath](https://www.sagemath.org/) Computer Algebra System (CAS), and the [notebook](https://www.sagemath.org/) is also available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch Tensor Version\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as fn\n",
    "import torch.optim as opt\n",
    "\n",
    "INPUT = torch.FloatTensor( [ 0.23, 0.82, 0.47 ]  )\n",
    "LABEL = target = torch.FloatTensor([1, 0])\n",
    "\n",
    "def run():\n",
    "   w1 = torch.tensor([ [0.1, 0.5, 0.3 ], [0.7, 0.2, 0.9 ], [0.4, 0.25, 0.75] ], requires_grad=True)\n",
    "   b1 = torch.tensor( [0.0, 0.0, 0.0], requires_grad=True)\n",
    "   w2 = torch.tensor([ [0.9, 0.6, 0.4 ], [0.3, 0.8, 0.7] ], requires_grad=True)\n",
    "   b2 = torch.tensor( [0.0, 0.0], requires_grad=True)\n",
    "   a1 = torch.relu(w1 @ INPUT + b1)\n",
    "   optimiser = opt.SGD((w1, b1, w2, b2), lr=0.01)\n",
    "   optimiser.zero_grad()\n",
    "   print('activated 1 = ', a1, \"Bias 2 = \", b2)\n",
    "   a2 = torch.sigmoid(w2 @ a1 + b2)\n",
    "   print('activated 2 = ', a2)\n",
    "   loss = fn.binary_cross_entropy(a2, LABEL, weight=None, reduction='mean')\n",
    "   print('Loss =', loss)\n",
    "   # loss.backward()\n",
    "   loss.backward()\n",
    "   print('grad W2 = \\n', w2.grad)\n",
    "   print('grad b2 = ', b2.grad)\n",
    "   print('grad W1 = \\n', w1.grad)\n",
    "   print('grad b1 = ', b1.grad)\n",
    "   optimiser.step()\n",
    "   print(\"Layer 1 Weights\\n\", w1.data)\n",
    "   print(\"Layer 1 Bias\", b1.data)\n",
    "   print('======================')\n",
    "   print(\"Layer 2 Weights\\n\", w2.data)\n",
    "   print(\"Layer 2 Bias\", b2.data)\n",
    "\n",
    "def main(argv):\n",
    "   run()\n",
    "```\n",
    "\n",
    "producing:\n",
    "\n",
    "```\n",
    "activated 1 =  tensor([0.5740, 0.7480, 0.6495], grad_fn=<ReluBackward0>) Bias 2 =  tensor([0., 0.], requires_grad=True)\n",
    "activated 2 =  tensor([0.7730, 0.7730], grad_fn=<SigmoidBackward>)\n",
    "Loss = tensor(0.8701, grad_fn=<BinaryCrossEntropyBackward>)\n",
    "grad W2 = \n",
    " tensor([[-0.0652, -0.0849, -0.0737],\n",
    "        [ 0.2218,  0.2891,  0.2510]])\n",
    "grad b2 =  tensor([-0.1135,  0.3865])\n",
    "grad W1 = \n",
    " tensor([[0.0032, 0.0113, 0.0065],\n",
    "        [0.0555, 0.1977, 0.1133],\n",
    "        [0.0518, 0.1846, 0.1058]])\n",
    "grad b1 =  tensor([0.0138, 0.2411, 0.2251])\n",
    "Layer 1 Weights\n",
    " tensor([[0.1000, 0.4999, 0.2999],\n",
    "        [0.6994, 0.1980, 0.8989],\n",
    "        [0.3995, 0.2482, 0.7489]])\n",
    "Layer 1 Bias tensor([-0.0001, -0.0024, -0.0023])\n",
    "======================\n",
    "Layer 2 Weights\n",
    " tensor([[0.9007, 0.6008, 0.4007],\n",
    "        [0.2978, 0.7971, 0.6975]])\n",
    "Layer 2 Bias tensor([ 0.0011, -0.0039])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mathematics\n",
    "### 3.1 Forward Pass\n",
    "We start by doing the forward pass and recording the numeric results for later use during backpropogation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activated 1 =  (0.574000000000000, 0.748000000000000, 0.649500000000000)\n",
      "activated 2 =  [0.772977357985251, 0.772986132033595]\n",
      "Loss =  0.870124846460152\n"
     ]
    }
   ],
   "source": [
    "%display latex\n",
    "viewer3D = 'threejs'\n",
    "clear_vars()\n",
    "#sigmoid(x) = 1 / (1 + exp(-x))\n",
    "def sigmoid(v): # MATLAB Symbolic AKA Maple is a little better here as functions work transparently with vectors and scalars\n",
    "    vv = [None] * len(v)\n",
    "    for i in range(0, len(v)):\n",
    "        vv[i] = 1 / (1 + exp(-v[i]))  \n",
    "    return vv        \n",
    "# ident(x) = x\n",
    "# relu = piecewise([[(-infinity, 0), 0], [(0, infinity), ident]])\n",
    "def relu(v):\n",
    "    vv = [None] * len(v)\n",
    "    for i in range(0, len(v)):\n",
    "        vv[i] = max(0,v[i])\n",
    "    return vector(SR, vv)        \n",
    "bce(l, y) = l*log(y) + (1 - l)*log(1 - y)\n",
    "\n",
    "def relun(x):\n",
    "    try:\n",
    "        return max(0,x)\n",
    "    except:\n",
    "        xx = [None] * len(x)\n",
    "        for i in range(0, len(x)):\n",
    "            xx[i] = max(0, x[i])\n",
    "        return xx    \n",
    "def sigmoidn(x):\n",
    "    try:\n",
    "        return 1 / (1 + exp(-x))\n",
    "    except:\n",
    "        xx = [None] * len(x)\n",
    "        for i in range(0, len(x)):\n",
    "            xx[i] = 1 / (1 + exp(-x[i]))  \n",
    "        return xx    \n",
    "def bcen(l, y):\n",
    "    return l*log(y) + (1 - l)*log(1 - y)\n",
    "            \n",
    "        \n",
    "Xn = vector([0.23, 0.82, 0.47 ])\n",
    "W1n = Matrix([ [0.1, 0.5, 0.3], [0.7, 0.2, 0.9], [0.4, 0.25, 0.75] ])\n",
    "W2n = Matrix([ [ 0.9, 0.6, 0.4],  [0.3, 0.8, 0.7] ])\n",
    "B1n = vector([0.0, 0.0, 0.0])\n",
    "B2n = vector([0.0, 0.0])\n",
    "l1n = 1.0\n",
    "l2n = 0.0\n",
    "\n",
    "Z1n = W1n * Xn + B1n;\n",
    "# Z1n.n(digits=5)\n",
    "A1n = relun(Z1n);\n",
    "print('activated 1 = ', A1n)\n",
    "Z2n = W2n * A1n + B2n\n",
    "A2n = sigmoidn(Z2n)\n",
    "print('activated 2 = ', A2n)\n",
    "# vpa([bce(l1n,A2n(1)), bce(l2n,A2n(2))], 5)\n",
    "En = -(bcen(l1n,A2n[0]) + bcen(l2n,A2n[1]))/2\n",
    "print('Loss = ', En)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen by comparing the feed forward results to the PyTorch ones everything matches. \n",
    "### 3.2 Backpropogation\n",
    "\n",
    "#### 3.2.1 Hidden Layer 2 Weight Gradient\n",
    "Next we start backpropogation by calculating the gradients for the second set of weights and biases. The error $E$ is expressed in terms \n",
    "of the output from the 2nd activation layer seen as probabilities $a_{2,i}$ and the labels for each output $l_i$: \n",
    "\n",
    "$E = -\\frac{1}{N} \\sum_{i=1}^{N} l_{i} \\cdot \\log \\left(a_{2,i})\\right)+\\left(1-l_i\\right) \\cdot \\log \\left(1-a_{2,i})\\right)$\n",
    "\n",
    "In this case $N = 2$ so $i \\in 1,2$ so the equation in this case is:\n",
    "\n",
    "$-\\frac{1}{2}(l_1\\log(a_{2,1}) + (1 - l_1)\\log(1 - a_{2,1}) + l_2\\log(a_{2,2}) + (1 - l_2)\\log(1 - a_{2,2})$.\n",
    "\n",
    "The 2nd activation layer is found in terms of the previous linear output $\\vec{z_2} = (z_{2,1}, z_{2,2})$ using the Sigmoid:\n",
    "\n",
    "$\\sigma_2(\\vec{z_2}) = (\\frac{1} {1 + e^{-z_{2,1}}}, \\frac{1} {1 + e^{-z_{2,2}}})$\n",
    "\n",
    "The linear output $z_2$ used as input to the sigmoid above is found in terms of the the output of the first activation layer $a_1 = (a_{1,1}, a_{1,2}, a_{1,3})$,\n",
    "the second weight matrix $W_2$ and the 2nd bias vector $\\vec{b_2}$:\n",
    "\n",
    "$\\mathcal{L}_2(\\vec{a_1}) = \\vec{z_2} = W_2 \\vec{a_1}^\\intercal + \\vec{b_2}$\n",
    "\n",
    "$E$ above is the result of function composition ie $(E \\circ \\sigma \\circ \\mathcal{L}_2)(\\vec{a_1})$\n",
    "We want the gradients w.r.t the weights in $W_2$ and the biases in $\\vec{b_2}$ $\\therefore$ we may use the (multivariable) chain rule:\n",
    "$D(E \\circ \\sigma \\circ \\mathcal{L}_2)(\\vec{a_1}) = D(E)[ \\sigma_2 \\circ \\mathcal{L}_2(\\sigma_1)] \\cdot D( \\sigma_2 \\circ \\mathcal{L}_2)[\\sigma_1] = \n",
    "J_{a_2}(E)[ \\sigma_2 \\circ \\mathcal{L}_2(\\sigma_1)] \\cdot J_{z_2}(\\sigma_2)[\\mathcal{L}_2(\\sigma_1)] \\cdot J_{W_2}(\\mathcal{L}_2)[\\vec{a_1}]$ \n",
    "\n",
    "where $\\vec{a1}$ is the output from activation layer 1 (relu).\n",
    "\n",
    "The square bracket notation $[expr]$ means \"evaluated at expr\", and we know what these values are from the forward pass. Also as we \n",
    "are doing multivariable differentation, the D's above can be seen as Jacobians (the $J$'s in the final part of the expression). In order to make\n",
    "it simpler to distinguish between weights and biases we will use separate expressions for the final term one with a Jacobian in terms of the\n",
    "weights and the other in terms of the bias, although computationally this is unnecessary as the output from a single can be reshaped appropriately.\n",
    "The Sagemath representation of the above follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Weights 2 Grad ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html><script type=\"math/tex; mode=display\">\\newcommand{\\Bold}[1]{\\mathbf{#1}}\\left(\\begin{array}{rrr}\n",
       "-0.065156 & -0.084906 & -0.073726 \\\\\n",
       "0.22185 & 0.28910 & 0.25103\n",
       "\\end{array}\\right)</script></html>"
      ],
      "text/plain": [
       "[-0.065156 -0.084906 -0.073726]\n",
       "[  0.22185   0.28910   0.25103]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Bias 2 Grad ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html><script type=\"math/tex; mode=display\">\\newcommand{\\Bold}[1]{\\mathbf{#1}}\\left(\\begin{array}{rr}\n",
       "-0.11351 & 0.38649\n",
       "\\end{array}\\right)</script></html>"
      ],
      "text/plain": [
       "[-0.11351  0.38649]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var('a_11 a_12 a_13 a_21 a_22 l_1 l_2 z_21 z_22', domain='real')\n",
    "var('w2_11 w2_12 w2_21 w2_22 w2_31 w2_32 b2_1 b2_2', domain='real')\n",
    "W2 = matrix(SR, 3, 2, [w2_11, w2_12,  w2_21, w2_22,  w2_31, w2_32])\n",
    "W2 = W2.transpose()\n",
    "B2 = vector(SR, (b2_1, b2_2))\n",
    "a_1 = vector(SR, (a_11, a_12, a_13))\n",
    "E = -(bce(l_1, a_21) + bce(l_2, a_22))/2\n",
    "JE = jacobian(E, (a_21, a_22)).substitute(a_21=A2n[0], a_22=A2n[1], l_1=l1n, l_2=l2n)\n",
    "z_2 = vector(SR, (z_21, z_22))\n",
    "a_2 = sigmoid(z_2)\n",
    "JSig = jacobian(a_2, (z_21, z_22)).substitute(z_21=Z2n[0], z_22=Z2n[1])\n",
    "Ll2 = W2*a_1 + B2\n",
    "Jlw2 = jacobian(Ll2, W2.list()).substitute(a_11=A1n[0], a_12=A1n[1], a_13=A1n[2])\n",
    "Jlb2 = jacobian(Ll2, B2.list())\n",
    "Dw2 = JE*JSig*Jlw2\n",
    "Dw2o = matrix(RR, [ Dw2[0][0:3], Dw2[0][3:]])\n",
    "Db2 = JE*JSig*Jlb2\n",
    "print('---------- Weights 2 Grad ---------------')\n",
    "# pretty_print(JE.n(digits=5),JSig.n(digits=5),Jlw2.n(digits=5),'=')\n",
    "# pretty_print(Dw2.n(digits=5))\n",
    "pretty_print(Dw2o.n(digits=5))\n",
    "print('----------Bias 2 Grad ---------------')\n",
    "# pretty_print(JE.n(digits=5),JSig.n(digits=5),Jlb2.n(digits=5),'=')\n",
    "pretty_print(Db2.n(digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight matrix gradient reported above corresponds to the gradient matrix for W2 reported by Pytorch in the PyTorch section above:\n",
    "> grad W2 = \n",
    ">  tensor([[-0.0652, -0.0849, -0.0737],\n",
    ">         [ 0.2218,  0.2891,  0.2510]])\n",
    "\n",
    "as does the bias gradient\n",
    "> grad b2 =  tensor([-0.1135,  0.3865])\n",
    "\n",
    "#### 3.2.2 Layer 1 Weight Gradient\n",
    "\n",
    "The next step is to find the gradients w.r.t the first layer weights and biases. First, the activation layer 1 whose output  was used in\n",
    "$\\mathcal{L}_2$ above can be expressed in terms of $\\vec{z_1} = (z_{1,1}, z_{1,2}, z_{1,3})$, the output of the first linear layer $\\mathcal{L}_1$:\n",
    "\n",
    "$\\vec{\\hat a_1} = (a_{1,1}, a_{1,2}, a_{1,3}) = relu_1(\\vec{z_1}) = relu(z_{1,1}, z_{1,2}, z_{1,3})$\n",
    "\n",
    "and $\\vec{z_1}$ is the output of the first linear layer $\\mathcal{L}_1(\\vec{x}) = W_1 \\vec{x} + \\vec{b_1}$ where $\\vec{x} = (x_1, x_2, x_3)$ is the \n",
    "original input. \n",
    "\n",
    "The function composition for the above is is $(relu \\circ \\mathcal{L}_1)(\\vec{x})$. Before applying the chain rule again however we need first note that\n",
    "the Jacobean in the chain rule composition in Section 3.2.1 above was in terms of the layer 2 weights so as to obtain the weights, but in order to continue \n",
    "backpropogation the  $J_{W_2}(\\mathcal{L}_2)[\\vec{\\hat a_1}]$ term in the chain rule expression \n",
    "$J(E)[ \\sigma_2 \\circ \\mathcal{L}_2(\\sigma_1)] \\cdot J(\\sigma_2)[\\mathcal{L}_2(\\sigma_1)] \\cdot J_{W_2}(\\mathcal{L}_2)[\\vec{\\hat a_1}]$ needs to \n",
    "change to $J_{a_1}(\\mathcal{L}_2)[\\vec{\\hat a_1}]$, that is to differentiate w.r.t the first layer activation output. This enables the chain differentiation to \n",
    "continue with the correct shapes for the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ll2 = W2*a_1 + B2\n",
    "Jla2 = jacobian(Ll2, (a_11, a_12, a_13)).substitute(w2_11=W2n[0][0], w2_21=W2n[0][1], w2_31=W2n[0][2], w2_12=W2n[1][0], w2_22=W2n[1][1], w2_32=W2n[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule evaluation can then be performed on the expression below:\n",
    "\n",
    "$D(E \\circ \\sigma \\circ \\mathcal{L}_2 \\circ relu \\circ \\mathcal{L}_1)(\\vec{x}) = J(E)[ \\sigma_2 \\circ \\mathcal{L}_2(\\sigma_1)] \\cdot J(\\sigma_2)[\\mathcal{L}_2(\\sigma_1)] \\cdot J_{\\vec{a_1}}(\\mathcal{L}_2)[\\vec{\\hat a_1}] \\cdot J(relu)[\\mathcal{L}_1(\\vec{x})] \\cdot J(\\mathcal{L}_1)[\\vec{x}]$\n",
    "\n",
    "(Note we assume the input domain to be positive for this example (which it is for the numeric values) in order to not have branched outputs for the relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Weights 1 Grad ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html><script type=\"math/tex; mode=display\">\\newcommand{\\Bold}[1]{\\mathbf{#1}}\\left(\\begin{array}{rrr}\n",
       "0.0031712 & 0.011306 & 0.0064802 \\\\\n",
       "0.055450 & 0.19769 & 0.11331 \\\\\n",
       "0.051782 & 0.18462 & 0.10582\n",
       "\\end{array}\\right)</script></html>"
      ],
      "text/plain": [
       "[0.0031712  0.011306 0.0064802]\n",
       "[ 0.055450   0.19769   0.11331]\n",
       "[ 0.051782   0.18462   0.10582]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Bias 1 Grad ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html><script type=\"math/tex; mode=display\">\\newcommand{\\Bold}[1]{\\mathbf{#1}}\\left(\\begin{array}{rrr}\n",
       "0.013788 & 0.24109 & 0.22514\n",
       "\\end{array}\\right)</script></html>"
      ],
      "text/plain": [
       "[0.013788  0.24109  0.22514]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var('x_1 x_2 x_3 z1_1 z1_2  z1_3', domain='positive')\n",
    "x = vector(SR, (x_1, x_2, x_3))\n",
    "var('w_11 w_12 w_13 w_21 w_22 w_23 w_31 w_32 w_33 b1_1 b1_2 b1_3', domain='real')\n",
    "W1 = matrix(SR, [ [w_11, w_12, w_13],  [w_21, w_22, w_23], [w_31, w_32, w_33] ])\n",
    "B1 = vector(SR, (b1_1, b1_2, b1_3))\n",
    "W1 = W1.transpose()\n",
    "Ll1 = W1*x + B1\n",
    "Jlw1 = jacobian(Ll1, W1.list()).substitute(x_1=Xn[0], x_2=Xn[1], x_3=Xn[2])\n",
    "Jlb1 = jacobian(Ll1, B1.list()).substitute(x_1=Xn[0], x_2=Xn[1], x_3=Xn[2])\n",
    "z_1 = vector(SR, (z1_1, z1_2, z1_3))\n",
    "A1 = relu(z_1)\n",
    "Ja1z = jacobian(A1, [z1_1, z1_2, z1_3])\n",
    "\n",
    "DW1 = JE*JSig*Jla2*Ja1z*Jlw1\n",
    "DW1o = matrix(RR, [ DW1[0][0:3], DW1[0][3:6], DW1[0][6:] ])\n",
    "DB1 = JE*JSig*Jla2*Ja1z*Jlb1\n",
    "print('---------- Weights 1 Grad ---------------')\n",
    "pretty_print(DW1o.n(digits=5))\n",
    "print('---------- Bias 1 Grad ---------------')\n",
    "pretty_print(DB1.n(digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight matrix gradient reported above corresponds to the gradient matrix for W2 reported by Pytorch in the PyTorch section above:\n",
    "\n",
    "> grad W1 = \n",
    ">  tensor([[0.0032, 0.0113, 0.0065],\n",
    ">         [0.0555, 0.1977, 0.1133],\n",
    ">         [0.0518, 0.1846, 0.1058]])\n",
    "\n",
    "> grad b1 =  tensor([0.0138, 0.2411, 0.2251])\n",
    "\n",
    "Note the approach here is not the way a real backpropogation algorithm would work, as its just a brute force application of the chain rule. \n",
    "See [\\[Margossian, Charles, 2018\\]](https://arxiv.org/pdf/1811.05031.pdf) for a description of the real back (and forward) propogation algorithms.\n",
    "\n",
    "Assuming a learning rate of 0.01 the weights can then be adjusted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html><script type=\"math/tex; mode=display\">\\newcommand{\\Bold}[1]{\\mathbf{#1}}\\left(\\begin{array}{rrr}\n",
       "0.0999682882189337 & 0.499886940606633 & 0.299935197664777 \\\\\n",
       "0.699445498381519 & 0.198023081186286 & 0.898866887997018 \\\\\n",
       "0.399482176579040 & 0.248153846933968 & 0.748941839096299\n",
       "\\end{array}\\right)</script></html>"
      ],
      "text/plain": [
       "[0.0999682882189337  0.499886940606633  0.299935197664777]\n",
       "[ 0.699445498381519  0.198023081186286  0.898866887997018]\n",
       "[ 0.399482176579040  0.248153846933968  0.748941839096299]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html><script type=\"math/tex; mode=display\">\\newcommand{\\Bold}[1]{\\mathbf{#1}}\\left(\\begin{array}{rrr}\n",
       "0.900651554982582 & 0.600849064681135 & 0.400737256029943 \\\\\n",
       "0.297781529801064 & 0.797109031866194 & 0.697489727536221\n",
       "\\end{array}\\right)</script></html>"
      ],
      "text/plain": [
       "[0.900651554982582 0.600849064681135 0.400737256029943]\n",
       "[0.297781529801064 0.797109031866194 0.697489727536221]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Weights 1')\n",
    "pretty_print(-0.01*DW1o + W1n)\n",
    "print('Weights 2')\n",
    "pretty_print(-0.01*Dw2o + W2n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which corresponds to the PyTorch calculated weights:\n",
    "\n",
    "> Layer 1 Weights\n",
    "\n",
    ">  tensor([[0.1000, 0.4999, 0.2999],\n",
    "\n",
    ">         [0.6994, 0.1980, 0.8989],\n",
    "\n",
    ">         [0.3995, 0.2482, 0.7489]])\n",
    "\n",
    "> Layer 1 Bias tensor([-0.0001, -0.0024, -0.0023])\n",
    "\n",
    "> ======================\n",
    "\n",
    "> Layer 2 Weights\n",
    "\n",
    ">  tensor([[0.9007, 0.6008, 0.4007],\n",
    "\n",
    ">         [0.2978, 0.7971, 0.6975]])\n",
    "\n",
    "> Layer 2 Bias tensor([ 0.0011, -0.0039])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Model Based Version\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as opt\n",
    "\n",
    "WEIGHTS1 = torch.FloatTensor([ [0.1, 0.5, 0.3 ], [0.7, 0.2, 0.9 ], [0.4, 0.25, 0.75] ])\n",
    "WEIGHTS2 = torch.FloatTensor([ [0.9, 0.6, 0.4 ], [0.3, 0.8, 0.7] ])\n",
    "TRAIN = torch.FloatTensor( [ 0.23, 0.82, 0.47 ]  )\n",
    "LABEL = target = torch.FloatTensor([1, 0])\n",
    "\n",
    "class SimplisticNN(nn.Module):\n",
    "   def __init__(self):\n",
    "      super(SimplisticNN, self).__init__()\n",
    "      self.hidden1 = nn.Linear(3, 2, bias=True)\n",
    "      self.hidden1.weight.data = WEIGHTS1\n",
    "      self.hidden1.bias.data = torch.FloatTensor( [0, 0, 0])\n",
    "      self.activation1 = torch.nn.ReLU()\n",
    "\n",
    "      self.hidden2 = nn.Linear(3, 2, bias=True)\n",
    "      self.hidden2.weight.data = WEIGHTS2\n",
    "      self.hidden2.bias.data = torch.FloatTensor([0, 0])\n",
    "      self.activation2 = nn.Sigmoid()\n",
    "      self.loss = nn.BCELoss(reduction='mean')\n",
    "\n",
    "   def forward(self, batch):\n",
    "      hidden1 = self.hidden1(batch)\n",
    "      activated1 = self.activation1(hidden1)\n",
    "      hidden2 = self.hidden2(activated1)\n",
    "      out = self.activation2(hidden2)\n",
    "      return out\n",
    "\n",
    "model = SimplisticNN()\n",
    "# model.train()\n",
    "y = model(TRAIN)\n",
    "output = model.loss(y, LABEL)\n",
    "# for param in model.parameters():\n",
    "#    print(str(param))\n",
    "optimiser = opt.SGD(model.parameters(), lr=0.01)\n",
    "optimiser.zero_grad()\n",
    "output.backward()\n",
    "print('grad W2 = \\n', model.hidden2.weight.grad)\n",
    "print('grad b2 = ', model.hidden2.bias.grad)\n",
    "print('grad W1 = \\n',model.hidden1.weight.grad)\n",
    "print('grad b1 = ', model.hidden1.bias.grad)\n",
    "optimiser.step()\n",
    "\n",
    "print('Loss =', output)\n",
    "print(\"Layer 1 Weights\\n\", model.hidden1.weight.data)\n",
    "print(\"Layer 1 Bias\\n\", model.hidden1.bias.data)\n",
    "print('======================')\n",
    "print(\"Layer 2 Weights\\n\", model.hidden2.weight.data)\n",
    "print(\"Layer 2 Bias\\n\", model.hidden2.bias.data)\n",
    "```\n",
    "\n",
    "producing:\n",
    "\n",
    "```\n",
    "grad W2 = \n",
    " tensor([[-0.0652, -0.0849, -0.0737],\n",
    "        [ 0.2218,  0.2891,  0.2510]])\n",
    "grad b2 =  tensor([-0.1135,  0.3865])\n",
    "grad W1 = \n",
    " tensor([[0.0032, 0.0113, 0.0065],\n",
    "        [0.0555, 0.1977, 0.1133],\n",
    "        [0.0518, 0.1846, 0.1058]])\n",
    "grad b1 =  tensor([0.0138, 0.2411, 0.2251])\n",
    "Loss = tensor(0.8701, grad_fn=<BinaryCrossEntropyBackward>)\n",
    "Layer 1 Weights\n",
    " tensor([[0.1000, 0.4999, 0.2999],\n",
    "        [0.6994, 0.1980, 0.8989],\n",
    "        [0.3995, 0.2482, 0.7489]])\n",
    "Layer 1 Bias\n",
    " tensor([-0.0001, -0.0024, -0.0023])\n",
    "======================\n",
    "Layer 2 Weights\n",
    " tensor([[0.9007, 0.6008, 0.4007],\n",
    "        [0.2978, 0.7971, 0.6975]])\n",
    "Layer 2 Bias\n",
    " tensor([ 0.0011, -0.0039])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TensorFlow 2 Versions\n",
    "### 5.1 Using GradientTape\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.losses as kl\n",
    "\n",
    "TRAIN = np.array( [ [0.23, 0.82, 0.47 ] ]  )\n",
    "LABEL = np.array([1.0, 0.0 ] )\n",
    "WEIGHTS1 = np.array([ [0.1, 0.5, 0.3 ], [0.7, 0.2, 0.9 ], [0.4, 0.25, 0.75]  ])\n",
    "WEIGHTS2 = np.array([ [0.9, 0.6, 0.4 ], [0.3, 0.8, 0.7] ])\n",
    "\n",
    "def run():\n",
    "   input = tf.transpose(tf.convert_to_tensor(TRAIN, dtype=tf.float32))\n",
    "   labels = tf.transpose(tf.convert_to_tensor(LABEL, dtype=tf.float32))\n",
    "   w1 = tf.Variable(tf.convert_to_tensor(WEIGHTS1, dtype=tf.float32))\n",
    "   w2 = tf.Variable(tf.convert_to_tensor(WEIGHTS2, dtype=tf.float32))\n",
    "   b1 = tf.Variable(tf.transpose(tf.convert_to_tensor([ [0.0, 0.0, 0.0] ], dtype=tf.float32)))\n",
    "   b2 = tf.Variable(tf.transpose(tf.convert_to_tensor( [ [0.0, 0.0] ], dtype=tf.float32)))\n",
    "   dependents = (w1, b1, w2, b2)\n",
    "   optimizer = tf.optimizers.SGD(0.01)\n",
    "   with tf.GradientTape(persistent=True) as dag:\n",
    "      dag.watch(w1)\n",
    "      dag.watch(w2)\n",
    "      dag.watch(b1)\n",
    "      dag.watch(b2)\n",
    "      z1 = tf.linalg.matmul(w1, input)\n",
    "      a1 = tf.keras.activations.relu(tf.math.add(z1, b1))\n",
    "      # dag.watch(a1)\n",
    "      z2 = tf.linalg.matmul(w2, a1)\n",
    "      a2 = tf.keras.activations.sigmoid(tf.math.add(z2, b2))\n",
    "      # dag.watch(a2)\n",
    "      # print(a2)\n",
    "      loss = tf.reduce_mean(kl.binary_crossentropy(labels, a2))\n",
    "      print(\"Loss =\", loss)\n",
    "   grad = dag.gradient(loss, dependents)\n",
    "   # print(str(grad))\n",
    "   optimizer.apply_gradients(zip(grad, dependents))\n",
    "   # print(dag.gradient(loss, dependents))\n",
    "   print(dag.gradient(loss, w1))\n",
    "   print(dag.gradient(loss, b1))\n",
    "   print('grad W2 = \\n', dag.gradient(loss, w2))\n",
    "   print('grad b2 = ', dag.gradient(loss, b2))\n",
    "   print('grad W1 = \\n', dag.gradient(loss, w2))\n",
    "   print('grad b1 = ', dag.gradient(loss, b1))\n",
    "   print(\"Layer 1 Weights\\n\", w1)\n",
    "   print(\"Layer 1 Bias\",  b1)\n",
    "   print(\"Layer 2 Weights\\n\", w2)\n",
    "   print(\"Layer 2 Bias\",  b2)\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU') # Disable GPU\n",
    "run()\n",
    "\n",
    "```\n",
    "\n",
    "producing:\n",
    "\n",
    "```\n",
    "Loss = tf.Tensor(0.870112, shape=(), dtype=float32)\n",
    "tf.Tensor(\n",
    "[[0.03767115 0.13430585 0.07698018]\n",
    " [0.04395014 0.15669179 0.08981115]\n",
    " [0.03453232 0.12311524 0.07056605]], shape=(3, 3), dtype=float32)\n",
    "tf.Tensor(\n",
    "[[0.16378762]\n",
    " [0.19108756]\n",
    " [0.15014054]], shape=(3, 1), dtype=float32)\n",
    "grad W2 = \n",
    " tf.Tensor(\n",
    "[[0.07834444 0.10209345 0.08864933]\n",
    " [0.07834699 0.10209677 0.08865222]], shape=(2, 3), dtype=float32)\n",
    "grad b2 =  tf.Tensor(\n",
    "[[0.13648857]\n",
    " [0.13649301]], shape=(2, 1), dtype=float32)\n",
    "grad W1 = \n",
    " tf.Tensor(\n",
    "[[0.07834444 0.10209345 0.08864933]\n",
    " [0.07834699 0.10209677 0.08865222]], shape=(2, 3), dtype=float32)\n",
    "grad b1 =  tf.Tensor(\n",
    "[[0.16378762]\n",
    " [0.19108756]\n",
    " [0.15014054]], shape=(3, 1), dtype=float32)\n",
    "Layer 1 Weights\n",
    " <tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
    "array([[0.09962329, 0.49865693, 0.29923022],\n",
    "       [0.69956046, 0.19843309, 0.89910185],\n",
    "       [0.3996547 , 0.24876885, 0.74929434]], dtype=float32)>\n",
    "Layer 1 Bias <tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
    "array([[-0.00163788],\n",
    "       [-0.00191088],\n",
    "       [-0.00150141]], dtype=float32)>\n",
    "Layer 2 Weights\n",
    " <tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
    "array([[0.89921653, 0.5989791 , 0.3991135 ],\n",
    "       [0.29921654, 0.79897904, 0.6991135 ]], dtype=float32)>\n",
    "Layer 2 Bias <tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
    "array([[-0.00136489],\n",
    "       [-0.00136493]], dtype=float32)>\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Using tf.keras\n",
    "Displaying the gradients is left as an exercise for the reader (i.e. I couldn't find any simple\n",
    "way of doing it).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "TRAIN = np.array( [ [ 0.23, 0.82, 0.47 ] ])\n",
    "LABEL = np.array( [ [ 1.0, 0.0 ] ])\n",
    "WEIGHTS1 = np.array([ [0.1, 0.5, 0.3 ], [0.7, 0.2, 0.9 ], [0.4, 0.25, 0.75] ])\n",
    "WEIGHTS2 = np.array([ [0.9, 0.6, 0.4 ], [0.3, 0.8, 0.7] ])\n",
    "\n",
    "def setup():\n",
    "   weights1 = tf.constant_initializer(WEIGHTS1)\n",
    "   weights2 = tf.constant_initializer(WEIGHTS2)\n",
    "   m = tf.keras.models.Sequential()\n",
    "   m.add(keras.layers.Dense(3, input_shape=(3,), kernel_initializer=weights1, use_bias=True, bias_initializer='zeros', activation='relu'))\n",
    "   m.add(keras.layers.Dense(2, kernel_initializer=weights2, use_bias=True, bias_initializer='zeros', activation='sigmoid'))\n",
    "   m.compile(loss='binary_crossentropy', optimizer='SGD')\n",
    "#   from tensorflow.keras.utils import plot_model\n",
    "#   plot_model(m, to_file='model.png', show_shapes=True, expand_nested=True)\n",
    "   return m\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU') # 2.1\n",
    "#my_devices = tf.config.experimental.list_physical_devices(device_type='CPU') #2.0\n",
    "#tf.config.experimental.set_visible_devices(devices= my_devices, device_type='CPU') #2.0\n",
    "model = setup()\n",
    "\n",
    "_ = model.fit(TRAIN, LABEL, batch_size=1, epochs=1, verbose=False)\n",
    "print(model.weights[0])\n",
    "print('----------------------------------------')\n",
    "print(model.weights[1])\n",
    "print('=========================================')\n",
    "print(model.weights[2])\n",
    "print('----------------------------------------')\n",
    "print(model.weights[3])\n",
    "\n",
    "```\n",
    "\n",
    "producing:\n",
    "\n",
    "```\n",
    "{'batch': 0, 'size': 1, 'loss': 0.8878588}\n",
    "<tf.Variable 'dense/kernel:0' shape=(3, 3) dtype=float32, numpy=\n",
    "array([[0.09959406, 0.49978882, 0.2994854 ],\n",
    "       [0.6985527 , 0.1992471 , 0.89816517],\n",
    "       [0.39917046, 0.24956846, 0.74894834]], dtype=float32)>\n",
    "----------------------------------------\n",
    "<tf.Variable 'dense/bias:0' shape=(3,) dtype=float32, numpy=array([-0.00176497, -0.00091817, -0.00223756], dtype=float32)>\n",
    "=========================================\n",
    "<tf.Variable 'dense_1/kernel:0' shape=(3, 2) dtype=float32, numpy=\n",
    "array([[0.90056026, 0.59685045],\n",
    "       [0.40028298, 0.2984092 ],\n",
    "       [0.80082756, 0.69534785]], dtype=float32)>\n",
    "----------------------------------------\n",
    "<tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([ 0.00071371, -0.00401219], dtype=float32)>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.0",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
